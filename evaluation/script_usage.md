# Script Usage

This document shows the list of scripts and their usage.
Also check the help of each scripts.

## install_jumanpp.sh

`install_jumanpp.sh` is a helper script to install Juman++.

Juman++ is neccessary to use `tokenizer_utils.Juman`, which will be used to tokenize data for Kyoto-U BERT.

The default install location is `$HOME/.local/usr`.
Modify the script as you want and set PATH.


## convert_dataset.py

`convert_dataset.py` is a script to preprocess datasets.
`run_evaluation.py` requires the dataset format produced by this script.

This script has `--seed` and `--split-rate` option to randomize train/dev/test set,
however, in our experiment we use default split (no option).

### example

```bash
# Amazon Review
# Raw data will be loaded from huggingface datasets hub.
python convert_dataset.py amazon -o ./amazon

# RCQA dataset
# Download raw data from http://www.cl.ecei.tohoku.ac.jp/rcqa/.
python convert_dataset.py rcqa -i ./all-v1.0.json.gz -o ./rcqa

# KUCI dataset
# Download raw data from https://nlp.ist.i.kyoto-u.ac.jp/?KUCI and untar.
python convert_dataset.py kuci -i ./KUCI/ -o ./kuci
```


### tokenize texts (wakati)

Some BERT models need texts in the dataset tokenized (wakati-gaki):

- NICT BERT: by MeCab (juman dic)
- Kyoto-U BERT: by Juman++

Use `--tokenize` option to tokenize text columns.
You need to install tokenizers to use.

Note that `run_evaluation.py` also has an option to tokenize text.

```bash
# tokenize with Juman++
python convert_dataset.py rcqa -i ./all-v1.0.json.gz --tokenize juman

# tokenize with MeCab (juman dic)
python convert_dataset.py rcqa -i ./all-v1.0.json.gz \
    --tokenize mecab --dicdir /var/lib/mecab/dic/juman-utf-8 --mecabrc /etc/mecabrc
```

### normalize texts

To check an impact of the normalization, we evaluate models with datasets whose texts are normalized.
`convert_dataset.py` has option for that.

Use `--word-form` option to apply sudachitra normalization to texts.
We used `normalized_and_surface` for our experiment.

```bash
python convert_dataset.py amazon --word-form normalized_and_surface
```


## run_evaluation.py

`run_evaluation.py` is a script to run a single evaluation (with single model, dataset, hyper-parameters).

Note:
- The model path for `--model_name_or_path` must contain `bert` to let `transformers.AutoModel` work correctly.
- You may need to clear huggingface datasets cache file before running this script:
    - Dataset preprocessing will generate a cache file with random hash due to the our non-picklable conversion.
    - The random hash become same if you use same seed due to the set_seed.

### example

Template

```bash
python ./run_evaluation.py \
    --model_name_or_path          [./path/to/model or name in huggingface-hub] \
    --from_pt                     [set true if load pytorch model] \
    --pretokenizer_name           [set "juman" or "mecab-juman" to tokenize text before using HF-tokenizer] \
    --tokenizer_name              [set "sudachi" to use SudachiTokenizer] \
    --word_form_type              [arg for sudachi tokenizer] \
    --split_unit_type             [arg for sudachi tokenizer] \
    --sudachi_vocab_file          [arg for sudachi tokenizer] \
    --dataset_name                ["amazon" or "rcqa" or "kuci"] \
    --dataset_dir                 [./path/to/dataset/dir] \
    --output_dir                  [./path/to/output] \
    --do_train                    [set to finetune model] \
    --do_eval                     [set to evaluate model with dev set] \
    --do_predict                  [set to evaluate model with test data] \
    --per_device_eval_batch_size  [evaluation batch size] \
    --per_device_train_batch_size [training batch size] \
    --learning_rate               [learning rate] \
    --num_train_epochs            [epochs to finetune] \
    --max_train_samples           [limit number of train samples (for test run)] \
    --max_val_samples             [limit number of val samples (for test run)] \
    --max_test_samples            [limit number of test samples (for test run)] \
```

Run finetuning with tohoku BERT and amazon dataset,
assuming dataset file (generated by `convert_dataset.py`) locates under `datasets/amazon/`.

```bash
python ./run_evaluation.py \
    --model_name_or_path          "cl-tohoku/bert-base-japanese-whole-word-masking" \
    --dataset_name                "amazon" \
    --dataset_dir                 ./datasets/amazon \
    --output_dir                  ./output/tohoku_amazon \
    --do_train                    \
    --per_device_eval_batch_size  64 \
    --per_device_train_batch_size 16 \
    --learning_rate               5e-5 \
    --num_train_epochs            4 \
    # --max_train_samples           100 \
    # --max_val_samples             100 \
    # --max_test_samples            100 \
```

Run prediction with NICT BERT and KUCI dataset.
Assume dataset is not tokenized.

```bash
python ./run_evaluation.py \
    --model_name_or_path          ./path/to/nict_bert/model \
    --pretokenizer_name           "mecab-juman" \
    --dataset_name                "kuci" \
    --dataset_dir                 ./datasets/kuci \
    --output_dir                  ./output/nict_kuci \
    --do_eval                     \
    --do_predict                  \
    --per_device_eval_batch_size  64 \
    --per_device_train_batch_size 16 \
    --learning_rate               5e-5 \
    --num_train_epochs            4 \
```

Run whole steps with chitra (normalized_and_surface) and RCQA dataset.

```bash
python ./run_evaluation.py \
    --model_name_or_path          ./path/to/chitra/model \
    --tokenizer_name              "sudachi" \
    --word_form_type              "normalized_and_surface" \
    --split_unit_type             "C" \
    --sudachi_vocab_file          ./path/to/vocab/file \
    --dataset_name                "rcqa" \
    --dataset_dir                 ./datasets/rcqa \
    --output_dir                  ./output/chitra_rcqa \
    --do_train                    \
    --do_eval                     \
    --do_predict                  \
    --per_device_eval_batch_size  64 \
    --per_device_train_batch_size 16 \
    --learning_rate               5e-5 \
    --num_train_epochs            4 \
```


## run_all.sh

`run_all.sh` is a script to run `run_evaluation.py` with different models and hyper parameters.

This assumes all model files are placed in the same directory (and named `bert` for `run_evaluation.py`).
You will need to modify some variables in the scripts for your environment.

Note: As a side effect, this will remove huggingface datasets cache files.

```bash
# template
./run_all.sh [dataset_name]

# run with amazon dataset
./run_all.sh amazon
```


## summary_results.py

`summary_results.py` is a script to collect and summarize metrics of
test results of models with each hyper-parameters.

It requires the input directory has a structure generated by `run_all.sh`, i.e.:

```
input_dir
├── [hyper-parameter dirs (ex. "5e-5_32_3")]
│   ├── [validation result file]
│   └── [test result file]
...
```


```bash
python ./summary_results.py amazon -i ./out/chitra_amazon
```