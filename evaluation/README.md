This directory contains scripts to evaluate BERT models.

# Script Usage

## install_jumanpp.sh

`install_jumanpp.sh` is a shell script to install Juman++.

Juman++ is neccessary to use `tokenizer_util.Juman`, which will be used to tokenize data for Kyoto-U BERT.


## convert_dataset.py

`convert_dataset.py` is a script to preprocess datasets.
`run_evaluation.py` assumes the dataset preprocessed by this.

### example

```bash
# Amazon Review
# Raw data will be loaded from huggingface datasets hub.
python convert_dataset.py amazon -o ./amazon

# RCQA dataset
# Download raw data from http://www.cl.ecei.tohoku.ac.jp/rcqa/.
python convert_dataset.py rcqa -i ./all-v1.0.json.gz -o ./rcqa

# KUCI dataset
# Download raw data from https://nlp.ist.i.kyoto-u.ac.jp/?KUCI and untar.
python convert_dataset.py kuci -i ./KUCI/ -o ./kuci
```


### tokenize texts (wakati)

Some BERT models need texts in the dataset tokenized (wakati-gaki):

- NICT BERT: by MeCab (juman dic)
- Kyoto-U BERT: by Juman++

Use `--tokenize` option to tokenize text columns.
You need to install tokenizers to use.

Note that `run_evaluation.py` also has an option to tokenize text.

```bash
# tokenize with Juman++
python convert_dataset.py rcqa -i ./all-v1.0.json.gz --tokenize juman

# tokenize with MeCab (juman dic)
python convert_dataset.py rcqa -i ./all-v1.0.json.gz \
    --tokenize mecab --dicdir /var/lib/mecab/dic/juman-utf-8 --mecabrc /etc/mecabrc
```

### normalize texts

To check an impact of the normalization, we evaluate models with datasets whose texts are normalized.
`convert_dataset.py` has option for that.

Use `--word-form` option to apply sudachitra normalization to texts.
We use `normalized_and_surface` for our experiment.

```bash
python convert_dataset.py amazon --word-form normalized_and_surface
```


## run_evaluation.py

`run_evaluation.py` is a script to run a single evaluation (with single model, dataset, hyper-parameters).

### example

Template

```bash
python ./run_evaluation.py \
    --model_name_or_path          [./path/to/model or name in huggingface-hub] \
    --from_pt                     [set true if load pytorch model] \
    --pretokenizer_name           [set "juman" or "mecab-juman" to tokenize text before using HF-tokenizer] \
    --tokenizer_name              [set "sudachi" to use SudachiTokenizer] \
    --word_form_type              [arg for sudachi tokenizer] \
    --split_unit_type             [arg for sudachi tokenizer] \
    --sudachi_vocab_file          [arg for sudachi tokenizer] \
    --dataset_name                ["amazon" or "rcqa" or "kuci"] \
    --dataset_dir                 [./path/to/dataset/dir] \
    --output_dir                  [./path/to/output] \
    --do_train                    [set to finetune model] \
    --do_eval                     [set to evaluate model with dev set] \
    --do_predict                  [set to evaluate model with test data] \
    --per_device_eval_batch_size  [evaluation batch size] \
    --per_device_train_batch_size [training batch size] \
    --learning_rate               [learning rate] \
    --num_train_epochs            [epochs to finetune] \
    --max_train_samples           [limit number of train samples (for test run)] \
    --max_val_samples             [limit number of val samples (for test run)] \
    --max_test_samples            [limit number of test samples (for test run)] \
```

Run finetuning with tohoku BERT and amazon dataset,
assuming dataset file (generated by `convert_dataset.py`) locates under `datasets/amazon/`.

```bash
python ./run_evaluation.py \
    --model_name_or_path          "cl-tohoku/bert-base-japanese-whole-word-masking" \
    --dataset_name                "amazon" \
    --dataset_dir                 ./datasets/amazon \
    --output_dir                  ./output/tohoku_amazon \
    --do_train                    \
    --per_device_eval_batch_size  64 \
    --per_device_train_batch_size 16 \
    --learning_rate               5e-5 \
    --num_train_epochs            4 \
    # --max_train_samples           100 \
    # --max_val_samples             100 \
    # --max_test_samples            100 \
```

Run prediction with NICT BERT and KUCI dataset.
Assume dataset is not tokenized.

```bash
python ./run_evaluation.py \
    --model_name_or_path          ./path/to/nict_bert/model \
    --pretokenizer_name           "mecab-juman" \
    --dataset_name                "kuci" \
    --dataset_dir                 ./datasets/kuci \
    --output_dir                  ./output/nict_kuci \
    --do_eval                     \
    --do_predict                  \
    --per_device_eval_batch_size  64 \
    --per_device_train_batch_size 16 \
    --learning_rate               5e-5 \
    --num_train_epochs            4 \
```

Run whole steps with chitra (normalized_and_surface) and RCQA dataset.

```bash
python ./run_evaluation.py \
    --model_name_or_path          ./path/to/chitra/model \
    --tokenizer_name              "sudachi" \
    --word_form_type              "normalized_and_surface" \
    --split_unit_type             "C" \
    --sudachi_vocab_file          ./path/to/vocab/file \
    --dataset_name                "rcqa" \
    --dataset_dir                 ./datasets/rcqa \
    --output_dir                  ./output/chitra_rcqa \
    --do_train                    \
    --do_eval                     \
    --do_predict                  \
    --per_device_eval_batch_size  64 \
    --per_device_train_batch_size 16 \
    --learning_rate               5e-5 \
    --num_train_epochs            4 \
```


## run_all.sh

`run_all.sh` is a script to run `run_evaluation.py` with different hyper parameters.

Before running this script, you will need to modify some variales in it.

Note: As a side effect, this will remove huggingface datasets cache files.

```bash
# run with amazon dataset
./finetune_all_models.sh amazon
```

